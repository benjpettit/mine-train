{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in os.listdir(\"../data/universal-trending-20171129-1548/\"):\n",
    "    with open(\"../data/universal-trending-20171129-1548/\" + file) as f:\n",
    "        for line in f:\n",
    "            docs.append(json.loads(line))\n",
    "doc_index = dict(zip([doc[\"id\"] for doc in docs], range(len(docs))))\n",
    "doc_ids = [doc[\"id\"] for doc in docs]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSubjectToken(s):\n",
    "    return \"subject\" + s.replace(\" \", \"\").replace(\",\", \"\")\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english',\n",
    "                         ngram_range = (1,2),\n",
    "                         token_pattern = \"[a-zA-Z]{2,}\",\n",
    "                         min_df = 100,\n",
    "                         max_df = 0.1)\n",
    "vectors = vectorizer.fit_transform([\" \".join([doc[\"title\"], doc[\"abstract\"]])  for doc in docs])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for v in vectorizer.vocabulary_.keys():\n",
    "#     if \"subject\" in v:\n",
    "#         print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(vectorizer.idf_, vectorizer.vocabulary_), key=lambda x: -x[0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "DITHERING_EPSILON = 10\n",
    "def dither(results):\n",
    "    ranks = np.arange(len(results))\n",
    "    ditherScores = np.log1p(ranks) + (np.log(DITHERING_EPSILON) * np.random.randn(len(results)))\n",
    "    return list(np.array(results)[np.argsort(ditherScores)])\n",
    "\n",
    "dither(list(enumerate(range(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "model = SGDClassifier(loss=\"log\")\n",
    "alreadySeen = set()\n",
    "# TODO: get batches\n",
    "# TODO: explore/exploit tradeoff\n",
    "# TODO: diterhing\n",
    "sampleWeights = {\n",
    "    0:0.01,\n",
    "    1:1.0\n",
    "}\n",
    "exploreProbability = .33\n",
    "\n",
    "def selectCandidate():\n",
    "    if not alreadySeen:\n",
    "#         print(\"--> Getting a random candidate\")\n",
    "        return random.choice(docs)\n",
    "    scores = model.predict_proba(vectors)[:,1]\n",
    "    if (random.random() < exploreProbability):\n",
    "        print(\"--> explore uncertian examples\")\n",
    "        scoreFunc = lambda x: np.abs(0.5-x)\n",
    "    else:\n",
    "        print(\"--> exploit our best guess\")\n",
    "        scoreFunc = lambda x: -x\n",
    "    rankedDocs = dither(sorted(zip(doc_ids, scores), key=lambda x: scoreFunc(x[1]))[:100])\n",
    "    for docId, score in rankedDocs:\n",
    "#         docId = result[0]\n",
    "#         score = result[1]\n",
    "        if docId not in alreadySeen:\n",
    "            print(\"--> score %s\" % score)\n",
    "            return docs[doc_index[docId]]\n",
    "    print(\"No unseen documents found\")\n",
    "    \n",
    "def handleResponse(x, docId):\n",
    "    if x != 0.5:\n",
    "#         print(docId, \":\", x)\n",
    "        model.partial_fit(vectors[doc_index[docId]], np.array([x]),\n",
    "                          classes=np.array([0,1]), sample_weight=np.array([sampleWeights[x]]))\n",
    "        getFeedback()\n",
    "\n",
    "def getFeedback():\n",
    "    candidate = selectCandidate()\n",
    "    print(candidate[\"title\"])\n",
    "    print(candidate[\"abstract\"])\n",
    "#     print(candidate[\"subjectArea\"])\n",
    "    alreadySeen.add(candidate[\"id\"])\n",
    "    interact(handleResponse, x=(0.0,1,0.5), docId=fixed(candidate[\"id\"]));\n",
    "getFeedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab = sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])\n",
    "\n",
    "def top_features():\n",
    "    return [(f[0][0], f[1]) for f in sorted(zip(sorted_vocab, model.coef_[0]), key=lambda x: -x[1])[:20]]\n",
    "\n",
    "def bottom_features():\n",
    "    return [(f[0][0], f[1]) for f in sorted(zip(sorted_vocab, model.coef_[0]), key=lambda x: x[1])[:20]]\n",
    "for f in top_features():\n",
    "    print(f)\n",
    "for f in bottom_features():\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_features())\n",
    "print(bottom_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_features():\n",
    "    return [(vocab[0], coef) for vocab, coef in zip(sorted_vocab, model.coef_[0]) if vocab[0].startswith(\"subject\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectCandidate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
